{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Sentence Highlighter Model to Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates the process of deploying a sentence highlighting model to Amazon SageMaker. This model can be used to identify and highlight important sentences in a given text.\n",
    "\n",
    "We will cover the following steps:\n",
    "1.  **Setup**: Importing necessary libraries and configuring logging.\n",
    "2.  **SageMaker Role**: Creating or retrieving an IAM role for SageMaker to access AWS resources.\n",
    "3.  **Model Packaging**: Preparing the model artifacts, inference script, and dependencies into a `model.tar.gz` file.\n",
    "4.  **Deployment**: Uploading the packaged model to S3 and deploying it as a SageMaker endpoint.\n",
    "5.  **Testing (Optional)**: How to invoke the deployed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import shutil\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "import json # Ensure json is imported as it's used in create_sagemaker_role\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "# from sagemaker import get_execution_role # This is often used, but the script uses a custom role creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Libraries imported and logging configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SageMaker IAM Role\n",
    "\n",
    "Amazon SageMaker needs permissions to access other AWS services, such as S3 (for model artifacts) and CloudWatch (for logs). We define a function `create_sagemaker_role` that either creates a new IAM role with the necessary permissions or fetches an existing one.\n",
    "\n",
    "**Permissions required:**\n",
    "*   `AmazonSageMakerFullAccess`: Allows SageMaker to manage training jobs, endpoints, etc.\n",
    "*   `AmazonS3FullAccess`: Allows SageMaker to read model data from S3 and write output. (Note: In a production environment, you should scope this down to specific buckets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sagemaker_role(role_name='SageMakerExecutionRoleScript'):\n",
    "    \"\"\"Create or get a SageMaker execution role with necessary permissions.\"\"\"\n",
    "    iam = boto3.client('iam')\n",
    "    account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "    role_arn = f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Checking for existing role: {role_name}\")\n",
    "        iam.get_role(RoleName=role_name)\n",
    "        logger.info(f\"Role {role_name} already exists. Using ARN: {role_arn}\")\n",
    "    except iam.exceptions.NoSuchEntityException:\n",
    "        logger.info(f\"Role {role_name} not found. Creating new role...\")\n",
    "        assume_role_policy_document = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n",
    "                    \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        try:\n",
    "            role_response = iam.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    "            )\n",
    "            role_arn = role_response['Role']['Arn']\n",
    "            logger.info(f\"Created new role: {role_name} with ARN: {role_arn}\")\n",
    "\n",
    "            policies = [\n",
    "                'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "                'arn:aws:iam::aws:policy/AmazonS3FullAccess' # Consider more restrictive policies for production\n",
    "            ]\n",
    "            for policy_arn_to_attach in policies:\n",
    "                iam.attach_role_policy(RoleName=role_name, PolicyArn=policy_arn_to_attach)\n",
    "                logger.info(f\"Attached policy {policy_arn_to_attach} to role {role_name}\")\n",
    "            \n",
    "            logger.info(\"Waiting 10 seconds for role to be fully available...\")\n",
    "            time.sleep(10) # IAM changes can take a moment to propagate\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create role or attach policies: {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get SageMaker role '{role_name}': {str(e)}\")\n",
    "        raise\n",
    "    return role_arn\n",
    "\n",
    "# Example of how to call it (optional: can be called later in the main execution block)\n",
    "# try:\n",
    "#     sagemaker_role_arn = create_sagemaker_role()\n",
    "#     logger.info(f\"Using SageMaker Role ARN: {sagemaker_role_arn}\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Role creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Packaging\n",
    "\n",
    "To deploy a model to SageMaker, we need to package it in a specific format. This typically involves creating a `model.tar.gz` file containing:\n",
    "*   The trained model artifact (e.g., `model.pt` or `pytorch_model.bin`).\n",
    "*   An inference script (commonly named `inference.py`) that SageMaker will use to load the model and make predictions. This script must define specific functions like `model_fn`, `input_fn`, `predict_fn`, and `output_fn`.\n",
    "*   A `requirements.txt` file listing any dependencies that need to be installed in the SageMaker serving container.\n",
    "\n",
    "The `prepare_model_package` function below automates this process. You will need to provide paths to your model file, your `inference.py` script, and your `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_package(source_model_path, inference_script_path, requirements_path, model_dir=\"model_package\"):\n",
    "    \"\"\"Prepare model package for deployment.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Preparing model package...\")\n",
    "        \n",
    "        # Clean up existing model directory\n",
    "        if os.path.exists(model_dir):\n",
    "            shutil.rmtree(model_dir)\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "        logger.info(f\"Using source model path: {source_model_path}\")\n",
    "        if not os.path.exists(source_model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found at {source_model_path}\")\n",
    "        \n",
    "        # Create code directory for inference script\n",
    "        code_dir = os.path.join(model_dir, \"code\")\n",
    "        os.makedirs(code_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy inference script to code directory\n",
    "        logger.info(f\"Using inference script path: {inference_script_path}\")\n",
    "        if not os.path.exists(inference_script_path):\n",
    "            raise FileNotFoundError(f\"Inference script not found at {inference_script_path}\")\n",
    "        shutil.copy(inference_script_path, code_dir)\n",
    "        \n",
    "        # Copy requirements.txt to code directory\n",
    "        logger.info(f\"Using requirements path: {requirements_path}\")\n",
    "        if not os.path.exists(requirements_path):\n",
    "            raise FileNotFoundError(f\"requirements.txt not found at {requirements_path}\")\n",
    "        shutil.copy(requirements_path, code_dir)\n",
    "        \n",
    "        # Copy model file to model directory (SageMaker expects the model file at the root of the tarball or in a specific structure)\n",
    "        # For PyTorchModel, placing it in the root of the tarball is common.\n",
    "        # The arcname in tar.add will determine its path in the archive.\n",
    "        target_model_filename = os.path.basename(source_model_path)\n",
    "        # shutil.copy(source_model_path, os.path.join(model_dir, target_model_filename)) # Not strictly needed to copy here if only adding to tarball from source_model_path\n",
    "\n",
    "        # Create model.tar.gz\n",
    "        output_tarball = \"model.tar.gz\"\n",
    "        if os.path.exists(output_tarball):\n",
    "            os.remove(output_tarball)\n",
    "            \n",
    "        logger.info(f\"Creating {output_tarball}...\")\n",
    "        with tarfile.open(output_tarball, \"w:gz\") as tar:\n",
    "            # Add model file to the root of the tarball\n",
    "            tar.add(source_model_path, arcname=target_model_filename)\n",
    "            # Add the code directory (containing inference.py and requirements.txt)\n",
    "            tar.add(code_dir, arcname=\"code\")\n",
    "        \n",
    "        logger.info(f\"Model package prepared successfully as {output_tarball}\")\n",
    "        return output_tarball\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing model package: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage (paths will need to be defined by the user)\n",
    "# try:\n",
    "#     # Define these paths according to your project structure\n",
    "#     model_file = \"path/to/your/opensearch-semantic-highlighter-v1.pt\" \n",
    "#     inference_py = \"path/to/your/inference.py\"\n",
    "#     requirements_txt = \"path/to/your/requirements.txt\"\n",
    "#\n",
    "#     if not (os.path.exists(model_file) and os.path.exists(inference_py) and os.path.exists(requirements_txt)):\n",
    "#         logger.warning(\"One or more required files (model, inference script, requirements) not found. Skipping example packaging.\")\n",
    "#     else:\n",
    "#         model_package_tarball = prepare_model_package(\n",
    "#             source_model_path=model_file,\n",
    "#             inference_script_path=inference_py,\n",
    "#             requirements_path=requirements_txt\n",
    "#         )\n",
    "#         logger.info(f\"Model package created: {model_package_tarball}\")\n",
    "#\n",
    "# except FileNotFoundError as fnf:\n",
    "#     logger.error(f\"Packaging failed due to missing file: {fnf}\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Packaging example failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Deployment to SageMaker\n",
    "\n",
    "Once the model package (`model.tar.gz`) is ready and the IAM role is set up, we can deploy the model to a SageMaker endpoint.\n",
    "\n",
    "This involves:\n",
    "1.  **Generating a unique endpoint name**: To avoid conflicts with existing endpoints.\n",
    "2.  **Uploading the model package to S3**: SageMaker loads models from S3.\n",
    "3.  **Creating a SageMaker `PyTorchModel` object**: This object points to the model data in S3 and specifies the inference script, framework versions, and IAM role.\n",
    "4.  **Deploying the model**: This provisions the necessary infrastructure (e.g., EC2 instances) and deploys the model container. This step can take several minutes.\n",
    "\n",
    "The `get_endpoint_name` function creates a unique name, and `deploy_sagemaker_model` handles the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoint_name():\n",
    "    \"\"\"Generate a unique endpoint name with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return f\"semantic-highlighter-{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_sagemaker_model(model_package_tarball, sagemaker_role_arn, \n",
    "                                 entry_point_script='inference.py', # Name of the script inside code/ within model.tar.gz\n",
    "                                 framework_version='2.0', # Specify a recent, valid PyTorch version\n",
    "                                 py_version='py310', # Specify a compatible Python version \n",
    "                                 instance_type='ml.m5.large', # Choose an appropriate instance type\n",
    "                                 initial_instance_count=1,\n",
    "                                 s3_bucket=None, # Optional: specify bucket, else uses default\n",
    "                                 s3_prefix='semantic-highlighter-script/model'):\n",
    "    \"\"\"Deploys the packaged model to a SageMaker endpoint.\"\"\"\n",
    "    try:\n",
    "        endpoint_name = get_endpoint_name()\n",
    "        logger.info(f\"Using endpoint name: {endpoint_name}\")\n",
    "        \n",
    "        session = sagemaker.Session()\n",
    "        if not s3_bucket:\n",
    "            s3_bucket = session.default_bucket()\n",
    "        logger.info(f\"Using S3 bucket: {s3_bucket} and prefix: {s3_prefix}\")\n",
    "\n",
    "        logger.info(f\"Uploading {model_package_tarball} to S3...\")\n",
    "        model_s3_uri = session.upload_data(path=model_package_tarball, bucket=s3_bucket, key_prefix=s3_prefix)\n",
    "        logger.info(f\"Model uploaded to S3: {model_s3_uri}\")\n",
    "        \n",
    "        logger.info(\"Creating SageMaker PyTorchModel...\")\n",
    "        # source_dir is not needed if entry_point and requirements.txt are inside the tarball's code/ directory.\n",
    "        # The entry_point argument should be the name of your inference script, e.g., 'inference.py'.\n",
    "        model = PyTorchModel(\n",
    "            model_data=model_s3_uri,\n",
    "            role=sagemaker_role_arn,\n",
    "            entry_point=entry_point_script, \n",
    "            framework_version=framework_version,\n",
    "            py_version=py_version,\n",
    "            sagemaker_session=session\n",
    "        )\n",
    "        logger.info(\"SageMaker PyTorchModel created successfully.\")\n",
    "        \n",
    "        logger.info(f\"Starting endpoint deployment for {endpoint_name} on {instance_type}. This may take 5-10 minutes...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        predictor = model.deploy(\n",
    "            initial_instance_count=initial_instance_count,\n",
    "            instance_type=instance_type,\n",
    "            endpoint_name=endpoint_name,\n",
    "            serializer=JSONSerializer(),\n",
    "            deserializer=JSONDeserializer(),\n",
    "            wait=True # Set to False if you don't want to wait for completion\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Endpoint {endpoint_name} deployed successfully in {end_time - start_time:.2f} seconds.\")\n",
    "        \n",
    "        # Save endpoint name for later use (e.g. testing, cleanup)\n",
    "        endpoint_file = 'sagemaker_endpoint_name.txt'\n",
    "        with open(endpoint_file, 'w') as f:\n",
    "            f.write(endpoint_name)\n",
    "        logger.info(f\"Endpoint name saved to {endpoint_file}\")\n",
    "            \n",
    "        return predictor, endpoint_name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Deployment failed: {str(e)}\")\n",
    "        # Consider adding cleanup for the S3 object if deployment fails mid-way\n",
    "        raise\n",
    "\n",
    "# Example usage (to be called in the main execution block)\n",
    "# try:\n",
    "#     # Assume model_package_tarball and sagemaker_role_arn are available\n",
    "#     if 'model_package_tarball' in locals() and 'sagemaker_role_arn' in locals():\n",
    "#         logger.info(\"Proceeding with deployment example...\")\n",
    "#         deployed_predictor, deployed_endpoint_name = deploy_sagemaker_model(\n",
    "#             model_package_tarball=model_package_tarball, \n",
    "#             sagemaker_role_arn=sagemaker_role_arn,\n",
    "#             instance_type='ml.t2.medium' # Use a cost-effective instance for testing\n",
    "#         )\n",
    "#         logger.info(f\"Endpoint '{deployed_endpoint_name}' is now active.\")\n",
    "#     else:\n",
    "#         logger.warning(\"Skipping deployment example as model package or role ARN is not defined.\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Deployment example failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites: Acquiring the Model Artifact\n",
    "\n",
    "Before you can run the deployment, you need the sentence highlighting model artifact. The example script refers to `opensearch-semantic-highlighter-v1.pt`.\n",
    "\n",
    "**How to obtain the model:**\n",
    "*   **Pre-trained Model:** If this is a publicly available pre-trained model from the OpenSearch project or another provider, include instructions or a link here. For example:\n",
    "    *   \"You can download the `opensearch-semantic-highlighter-v1.pt` model from [link to model repository/S3 bucket if available].\"\n",
    "    *   \"This model is typically packaged with [specific OpenSearch distribution/plugin version].\"\n",
    "*   **Training your own:** If users are expected to train their own model, provide a brief pointer:\n",
    "    *   \"If you have trained your own sentence highlighting model, ensure it's saved in PyTorch's `.pt` format. You might use `torch.save(model, 'your_model.pt')` or `torch.jit.save(scripted_model, 'your_model.pt')`.\"\n",
    "\n",
    "**For this notebook, you must:**\n",
    "1.  Obtain the `opensearch-semantic-highlighter-v1.pt` file (or your own equivalent `.pt` model file).\n",
    "2.  Update the `source_model_path` variable in the \"Run SageMaker Deployment\" section (Step 5) to point to the correct location of your model file on your local system.\n",
    "\n",
    "*(Developer Note: Please replace the placeholder text above with actual links or specific instructions for acquiring the `opensearch-semantic-highlighter-v1.pt` model if this information is available. If not, the current text provides general guidance.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on `inference.py`\n",
    "\n",
    "The deployment process relies on an `inference.py` script, which tells SageMaker how to load your model and make predictions. A template `inference.py` has been created in the same directory as this notebook (`docs/source/examples/inference.py`).\n",
    "\n",
    "**Key components of `inference.py`:**\n",
    "*   **`model_fn(model_dir)`**: Loads your trained model. It expects the model artifacts (like `.pt` file, `config.json`, `vocab.txt`) to be in `model_dir` after SageMaker unpacks `model.tar.gz`.\n",
    "*   **`input_fn(request_body, request_content_type)`**: Deserializes incoming request data.\n",
    "*   **`predict_fn(input_data, model)`**: Performs inference using the loaded model and processed input. **You will likely need to customize this function based on your specific model's sentence highlighting logic.**\n",
    "*   **`output_fn(prediction, response_content_type)`**: Serializes the prediction result to send back in the HTTP response.\n",
    "\n",
    "**Before running the deployment:**\n",
    "1.  **Review `docs/source/examples/inference.py`**.\n",
    "2.  **Customize `predict_fn`** and potentially `model_fn` if your model loading or inference logic is different from the template.\n",
    "3.  Ensure the `inference_script_path` variable in the 'Run SageMaker Deployment' section below points to your finalized `inference.py` script. The default path in the notebook will point to `docs/source/examples/inference.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `requirements.txt` for the SageMaker Environment\n",
    "\n",
    "The `inference.py` script might depend on specific Python packages (e.g., `torch`, `transformers`). You need to list these in a `requirements.txt` file, which will be included in the `model.tar.gz` package. SageMaker will use this file to install the necessary dependencies in the serving container.\n",
    "\n",
    "A template `requirements.txt` has been created in the same directory as this notebook (`docs/source/examples/requirements.txt`).\n",
    "\n",
    "**Before running the deployment:**\n",
    "1.  **Review `docs/source/examples/requirements.txt`**.\n",
    "2.  **Add or modify any dependencies** to match those required by your `inference.py` script and model.\n",
    "3.  Ensure the `requirements_path` variable in the 'Run SageMaker Deployment' section below points to your finalized `requirements.txt` file. The default path in the notebook will point to `docs/source/examples/requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run SageMaker Deployment\n",
    "\n",
    "Now we'll bring it all together. The cell below orchestrates the steps:\n",
    "1.  **Define Paths**: You **must** update the `source_model_path`, `inference_script_path`, and `requirements_path` variables to point to your files.\n",
    "2.  **Prepare Model Package**: Calls `prepare_model_package` to create `model.tar.gz`.\n",
    "3.  **Create/Get SageMaker Role**: Calls `create_sagemaker_role` to ensure you have the necessary IAM permissions.\n",
    "4.  **Deploy Model**: Calls `deploy_sagemaker_model` to deploy the packaged model to a SageMaker endpoint.\n",
    "\n",
    "**Important Considerations:**\n",
    "*   **File Paths**: Ensure the paths to your model (`.pt` file), `inference.py`, and `requirements.txt` are correct. For this example, it's assumed these files are located relative to the notebook, but you can use absolute paths.\n",
    "*   **Instance Type**: The `instance_type` in `deploy_sagemaker_model` (e.g., `ml.m5.large`, `ml.g4dn.xlarge`) should be chosen based on your model's requirements (CPU/GPU, memory, etc.) and budget. GPU instances like `g4dn` are suitable for larger transformer models if your `inference.py` leverages the GPU. For CPU-based inference or smaller models, `m5` instances can be more cost-effective.\n",
    "*   **AWS Permissions**: Ensure the AWS credentials used by `boto3` (implicitly configured via AWS CLI, environment variables, or IAM roles for SageMaker notebook instances) have permissions to:\n",
    "    *   Create and manage IAM roles (if the role doesn't exist).\n",
    "    *   List, create, and put objects in S3.\n",
    "    *   Create and manage SageMaker endpoints and models.\n",
    "*   **Region**: The SageMaker deployment will occur in the AWS region configured for your `boto3` session. You can explicitly set this if needed, e.g., `boto3.setup_default_session(region_name='us-west-2')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration: User needs to set these paths ---\n",
    "# Option 1: Place your files in the same directory as this notebook\n",
    "# and uncomment the lines below.\n",
    "# current_dir = os.getcwd() \n",
    "# source_model_path = os.path.join(current_dir, \"opensearch-semantic-highlighter-v1.pt\")\n",
    "# inference_script_path = os.path.join(current_dir, \"inference.py\")\n",
    "# requirements_path = os.path.join(current_dir, \"requirements.txt\")\n",
    "\n",
    "# Option 2: Specify absolute or relative paths directly.\n",
    "# PLEASE UPDATE THESE PATHS\n",
    "source_model_path = \"path/to/your/opensearch-semantic-highlighter-v1.pt\" \n",
    "inference_script_path = \"path/to/your/inference.py\" # This will be created in a later step by the notebook\n",
    "requirements_path = \"path/to/your/requirements.txt\" # This will be created in a later step by the notebook\n",
    "    \n",
    "# Choose your desired SageMaker instance type\n",
    "# For CPU-only: 'ml.m5.large', 'ml.c5.large', etc.\n",
    "# For GPU-enabled: 'ml.g4dn.xlarge', 'ml.g5.xlarge', etc. (ensure your inference.py uses the GPU)\n",
    "sagemaker_instance_type = 'ml.m5.large' \n",
    "# --- End Configuration ---\n",
    "\n",
    "# Variables to store results from steps\n",
    "model_package_archive = None\n",
    "sagemaker_role = None\n",
    "deployed_endpoint = None\n",
    "predictor_instance = None\n",
    "\n",
    "try:\n",
    "    # Validate essential paths before starting\n",
    "    # We will create dummy inference.py and requirements.txt later if they don't exist,\n",
    "    # but the model path MUST be provided by the user.\n",
    "    if not os.path.exists(source_model_path):\n",
    "        logger.error(f\"Source model file not found at: {source_model_path}\")\n",
    "        logger.error(\"Please update 'source_model_path' to point to your .pt model file.\")\n",
    "        raise FileNotFoundError(f\"Model file not found: {source_model_path}\")\n",
    "\n",
    "    # For the purpose of this notebook, we'll create placeholder inference.py and requirements.txt\n",
    "    # if they don't exist at the specified paths. Users should replace these with their actual files.\n",
    "    if not os.path.exists(inference_script_path):\n",
    "        logger.warning(f\"Inference script not found at {inference_script_path}. A template will be created.\")\n",
    "        # In a real scenario, the user provides this. Here, we'll create it in a later notebook step.\n",
    "        # For now, we might just create a dummy file to allow packaging to proceed if this cell is run early.\n",
    "        # However, the real template creation is a separate plan step.\n",
    "        # For this cell, we'll assume it will be created before prepare_model_package is called.\n",
    "        pass # Actual creation is step 7\n",
    "\n",
    "    if not os.path.exists(requirements_path):\n",
    "        logger.warning(f\"Requirements.txt not found at {requirements_path}. A template will be created.\")\n",
    "        # Similar to inference.py, actual creation is step 8\n",
    "        pass\n",
    "\n",
    "    logger.info(\"---- Step 1: Preparing Model Package ----\")\n",
    "    # Ensure the dummy/template files exist before packaging if running this cell directly after defining paths\n",
    "    # This will be handled by creating the actual files in plan steps 7 and 8.\n",
    "    # For now, the user must ensure these paths are valid or wait for those steps.\n",
    "    model_package_archive = prepare_model_package(\n",
    "        source_model_path=source_model_path,\n",
    "        inference_script_path=inference_script_path, # Path to user's or template script\n",
    "        requirements_path=requirements_path     # Path to user's or template requirements\n",
    "    )\n",
    "    logger.info(f\"Model package created: {model_package_archive}\")\n",
    "\n",
    "    logger.info(\"---- Step 2: Creating/Getting SageMaker IAM Role ----\")\n",
    "    sagemaker_role = create_sagemaker_role(role_name='MySageMakerExecutionRoleSH') # Use a specific role name\n",
    "    logger.info(f\"Using SageMaker Role ARN: {sagemaker_role}\")\n",
    "\n",
    "    logger.info(\"---- Step 3: Deploying Model to SageMaker ----\")\n",
    "    predictor_instance, deployed_endpoint = deploy_sagemaker_model(\n",
    "        model_package_tarball=model_package_archive, \n",
    "        sagemaker_role_arn=sagemaker_role,\n",
    "        instance_type=sagemaker_instance_type \n",
    "        # entry_point, framework_version, py_version use defaults from function definition\n",
    "    )\n",
    "    \n",
    "    logger.info(\"----------------------------------------------------\")\n",
    "    logger.info(f\"SageMaker Endpoint Deployed Successfully!\")\n",
    "    logger.info(f\"Endpoint Name: {deployed_endpoint}\")\n",
    "    logger.info(f\"Access your endpoint using the 'predictor_instance' object or AWS SDK.\")\n",
    "    logger.info(\"----------------------------------------------------\")\n",
    "    \n",
    "except FileNotFoundError as fnf_error:\n",
    "    logger.error(f\"Deployment process failed due to a missing file: {fnf_error}\")\n",
    "    logger.error(\"Please check the paths provided and ensure all required files exist.\")\n",
    "except Exception as main_error:\n",
    "    logger.error(f\"An error occurred during the deployment process: {main_error}\", exc_info=True)\n",
    "    # Consider adding cleanup for partially created resources if applicable, e.g. S3 objects or roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing and Cleanup (Important)\n",
    "\n",
    "### Testing the Endpoint\n",
    "You can test your deployed endpoint using the `predictor_instance` object returned by the `deploy` call:\n",
    "\n",
    "```python\n",
    "# Example payload - this depends on your model's input_fn in inference.py\n",
    "# For a sentence highlighter, it might expect a JSON object with a text field.\n",
    "# payload = {\"text\": \"This is a sample sentence. This is another one.\"}\n",
    "# try:\n",
    "#     if predictor_instance:\n",
    "#         response = predictor_instance.predict(payload)\n",
    "#         logger.info(f\"Prediction response: {response}\")\n",
    "#     else:\n",
    "#         logger.warning(\"Predictor instance is not available. Deployment might have failed.\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Error during prediction: {e}\")\n",
    "```\n",
    "\n",
    "### Cleaning Up\n",
    "SageMaker endpoints incur costs while they are running. **It's crucial to delete the endpoint when you are done to avoid ongoing charges.**\n",
    "\n",
    "```python\n",
    "# --- IMPORTANT: Delete the endpoint to avoid charges ---\n",
    "# try:\n",
    "#     if predictor_instance:\n",
    "#         endpoint_to_delete = predictor_instance.endpoint_name\n",
    "#         predictor_instance.delete_endpoint() # Deletes the endpoint and endpoint configuration\n",
    "#         logger.info(f\"Endpoint '{endpoint_to_delete}' deleted successfully.\")\n",
    "#\n",
    "#         # Optionally, delete the model from SageMaker\n",
    "#         # model_name = predictor_instance.model_name # This might not be directly available on predictor\n",
    "#         # To get model name, you might need to list models or infer from endpoint name if consistent\n",
    "#         # sagemaker_session = sagemaker.Session()\n",
    "#         # sagemaker_session.delete_model(model_name)\n",
    "#         # logger.info(f\"Model '{model_name}' deleted successfully.\")\n",
    "#\n",
    "#         # Clean up the S3 bucket objects (optional)\n",
    "#         # if model_package_archive and sagemaker_role: # check if these were created\n",
    "#         #    s3_uri_parts = model_s3_uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "#         #    bucket_name = s3_uri_parts[0]\n",
    "#         #    key_prefix = \"/\".join(s3_uri_parts[1:])\n",
    "#         #    s3 = boto3.resource('s3')\n",
    "#         #    bucket = s3.Bucket(bucket_name)\n",
    "#         #    bucket.objects.filter(Prefix=key_prefix).delete()\n",
    "#         #    logger.info(f\"Deleted model artifacts from S3: s3://{bucket_name}/{key_prefix}\")\n",
    "#\n",
    "#         # Remove the locally created model.tar.gz and endpoint name file\n",
    "#         if os.path.exists('model.tar.gz'):\n",
    "#             os.remove('model.tar.gz')\n",
    "#         if os.path.exists('sagemaker_endpoint_name.txt'):\n",
    "#             os.remove('sagemaker_endpoint_name.txt')\n",
    "#\n",
    "#     elif deployed_endpoint: # If predictor_instance is None but we have the name\n",
    "#         logger.warning(f\"Predictor instance not found. Attempting to delete endpoint '{deployed_endpoint}' by name.\")\n",
    "#         sm_client = boto3.client('sagemaker')\n",
    "#         sm_client.delete_endpoint(EndpointName=deployed_endpoint)\n",
    "#         logger.info(f\"Endpoint '{deployed_endpoint}' deletion initiated.\")\n",
    "#         # Note: Deleting endpoint config and model would require more logic if only name is available\n",
    "#     else:\n",
    "#         logger.info(\"No active endpoint or endpoint name found to delete.\")\n",
    "#\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Error during cleanup: {e}\")\n",
    "```\n",
    "**Note:** The cleanup script above for S3 and SageMaker models is more advanced. The most critical part is `predictor_instance.delete_endpoint()`. For full cleanup, you would also delete the SageMaker Model and the S3 artifacts if they are no longer needed."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
